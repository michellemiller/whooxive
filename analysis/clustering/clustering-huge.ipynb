{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I predict the existence of subfields with some cool unsupervised learning algorithm? \n",
    "\n",
    "For starters, let's just use regular n-grams. A more advanced version would be to look for noun phrases or J&K POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "%matplotlib inline\n",
    "# import matplotlib.pyplot as plt \n",
    "import time\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "# from nltk.corpus import stopwords\n",
    "# s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'acc-phys', u'adap-org', u'alg-geom', u'ao-sci', u'astro-ph', u'astro-ph.CO', u'astro-ph.EP', u'astro-ph.GA', u'astro-ph.HE', u'astro-ph.IM', u'astro-ph.SR', u'atom-ph', u'bayes-an', u'chao-dyn', u'chem-ph', u'comp-gas', u'cond-mat', u'cond-mat.dis-nn', u'cond-mat.mes-hall', u'cond-mat.mtrl-sci', u'cond-mat.other', u'cond-mat.quant-gas', u'cond-mat.soft', u'cond-mat.stat-mech', u'cond-mat.str-el', u'cond-mat.supr-con', u'cs.AI', u'cs.AR', u'cs.CC', u'cs.CE', u'cs.CG', u'cs.CL', u'cs.CR', u'cs.CV', u'cs.CY', u'cs.DB', u'cs.DC', u'cs.DL', u'cs.DM', u'cs.DS', u'cs.ET', u'cs.FL', u'cs.GL', u'cs.GR', u'cs.GT', u'cs.HC', u'cs.IR', u'cs.IT', u'cs.LG', u'cs.LO', u'cs.MA', u'cs.MM', u'cs.MS', u'cs.NA', u'cs.NE', u'cs.NI', u'cs.OH', u'cs.PF', u'cs.PL', u'cs.RO', u'cs.SC', u'cs.SD', u'cs.SE', u'cs.SI', u'cs.SY', u'dg-ga', u'funct-an', u'gr-qc', u'hep-ex', u'hep-lat', u'hep-ph', u'hep-th', u'math-ph', u'math.AC', u'math.AG', u'math.AP', u'math.AT', u'math.CA', u'math.CO', u'math.CT', u'math.CV', u'math.DG', u'math.DS', u'math.FA', u'math.GM', u'math.GN', u'math.GR', u'math.GT', u'math.HO', u'math.IT', u'math.KT', u'math.LO', u'math.MG', u'math.MP', u'math.NA', u'math.NT', u'math.OA', u'math.OC', u'math.PR', u'math.QA', u'math.RA', u'math.RT', u'math.SG', u'math.SP', u'math.ST', u'mtrl-th', u'nlin.AO', u'nlin.CD', u'nlin.CG', u'nlin.PS', u'nlin.SI', u'nucl-ex', u'nucl-th', u'patt-sol', u'physics.acc-ph', u'physics.ao-ph', u'physics.atm-clus', u'physics.atom-ph', u'physics.bio-ph', u'physics.chem-ph', u'physics.class-ph', u'physics.comp-ph', u'physics.data-an', u'physics.ed-ph', u'physics.flu-dyn', u'physics.gen-ph', u'physics.geo-ph', u'physics.hist-ph', u'physics.ins-det', u'physics.med-ph', u'physics.optics', u'physics.plasm-ph', u'physics.pop-ph', u'physics.soc-ph', u'physics.space-ph', u'plasm-ph', u'q-alg', u'q-bio', u'q-bio.BM', u'q-bio.CB', u'q-bio.GN', u'q-bio.MN', u'q-bio.NC', u'q-bio.OT', u'q-bio.PE', u'q-bio.QM', u'q-bio.SC', u'q-bio.TO', u'q-fin.CP', u'q-fin.EC', u'q-fin.GN', u'q-fin.MF', u'q-fin.PM', u'q-fin.PR', u'q-fin.RM', u'q-fin.ST', u'q-fin.TR', u'quant-ph', u'solv-int', u'stat.AP', u'stat.CO', u'stat.ME', u'stat.ML', u'stat.OT', u'stat.TH', u'supr-con']\n"
     ]
    }
   ],
   "source": [
    "# What are the available categories?\n",
    "categories = sorted([x.name for x in session.query(Category)])\n",
    "print categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435.189222097\n"
     ]
    }
   ],
   "source": [
    "abstract_all_tmp = {'category': [], 'abstract': []}\n",
    "# category_list = sorted(['atom-ph', 'quant-ph', 'optics', 'nlin', 'str-el', 'stat'])\n",
    "category_list = sorted(['quant-ph', 'str-el', 'hep-', 'mtrl-sci', 'supr-con'])\n",
    "category_len = len(category_list)\n",
    "\n",
    "start = time.time()\n",
    "for item in category_list:\n",
    "    query = session.query(Article_Category)\\\n",
    "                        .join(Category)\\\n",
    "                        .join(Article)\\\n",
    "                        .filter(Category.name.like('%' + item + '%'))\n",
    "#     query = session.query(Article_Category)\\\n",
    "#                         .join(Category)\\\n",
    "#                         .join(Article)\\\n",
    "#                         .filter(Category.name.like('%' + item + '%'))\n",
    "    result = [' '.join(x.article.abstract.split()) for x in query]\n",
    "    abstract_all_tmp['abstract'].extend(result)\n",
    "    abstract_all_tmp['category'].extend([item]*len(result))\n",
    "print time.time() - start\n",
    "# for item in query:\n",
    "#     abstract_all['category'].append(item.category.name)\n",
    "#     abstract_all['abstract'].append(' '.join(item.article.abstract.split()))\n",
    "# print time.time() - start\n",
    "# abstract_all['atom-ph'] = [x.article.abstract for x in query.all()]\n",
    "# session.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supr-con       24002\n",
      "str-el         35651\n",
      "hep-           217391\n",
      "mtrl-sci       38442\n",
      "quant-ph       60594\n",
      "Total          376080\n"
     ]
    }
   ],
   "source": [
    "# Breakdown of categories?\n",
    "count = Counter(abstract_all_tmp['category'])\n",
    "for key, val in count.iteritems():\n",
    "    print '{:<15}{}'.format(key, val)\n",
    "print '{:<15}{}'.format('Total', len(abstract_all_tmp['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261661\n",
      "261661\n"
     ]
    }
   ],
   "source": [
    "##Oops! How many overlapping articles do we have? I forgot that arXiv categories aren't unique.\n",
    "# Let's remove all duplicates.\n",
    "# This is slow but I am tired.\n",
    "\n",
    "counter_duplicate = Counter(abstract_all_tmp['abstract'])\n",
    "\n",
    "abstract_all = {'category': [], 'abstract': []}\n",
    "for cat, abstract in itertools.izip(abstract_all_tmp['category'], abstract_all_tmp['abstract']):\n",
    "    if counter_duplicate[abstract] == 1:\n",
    "        abstract_all['category'].append(cat)\n",
    "        abstract_all['abstract'].append(abstract)\n",
    "print len(abstract_all['category'])\n",
    "print len(abstract_all['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supr-con       13400\n",
      "str-el         20290\n",
      "hep-           144269\n",
      "mtrl-sci       31617\n",
      "quant-ph       52085\n",
      "Total          261661\n"
     ]
    }
   ],
   "source": [
    "# Breakdown of categories? That's a lot of repetition!!!\n",
    "count = Counter(abstract_all['category'])\n",
    "for key, val in count.iteritems():\n",
    "    print '{:<15}{}'.format(key, val)\n",
    "print '{:<15}{}'.format('Total', len(abstract_all['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train on 80% of the data. Random_state ensures that we always get the same result.\n",
    "x_train, x_test, y_train, y_test = train_test_split(abstract_all['abstract'],\n",
    "                                                    abstract_all['category'],\n",
    "                                                    random_state=42,\n",
    "                                                    train_size=0.8)\n",
    "\n",
    "counter_train = Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I lied, I'm starting with supervised learning (as a comparison). We're looking at ~60-70% accuracy for these cateogories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804.721962929\n",
      "48.4372420311\n"
     ]
    }
   ],
   "source": [
    "#SVC(kernel='linear') is good\n",
    "clf_supervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "#                            ('clf', LinearSVC())])\n",
    "                           ('clf', LinearSVC(C=1,penalty='l1',dual=False,))])\n",
    "start = time.time()\n",
    "clf_supervised.fit(x_train, y_train)\n",
    "print time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "predict = clf_supervised.predict(x_test)\n",
    "print time.time() - start\n",
    "#print text_abstract_clf.predict(train_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       hep-       0.98      0.98      0.98     28613\n",
      "   mtrl-sci       0.88      0.91      0.90      6309\n",
      "   quant-ph       0.93      0.92      0.92     10547\n",
      "     str-el       0.85      0.80      0.82      4182\n",
      "   supr-con       0.90      0.88      0.89      2682\n",
      "\n",
      "avg / total       0.94      0.94      0.94     52333\n",
      "\n",
      "Accuracy score: 0.94\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict))\n",
    "print('Accuracy score: %0.2f' % accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category hep-:\n",
      "qcd, quark, holographic, hadron, neutrino, hep, gev, lhc, meson, ads, quarks, cosmological, brane, yang mills, branes, dark matter, mesons, hadronic, quantum group, parton\n",
      "\n",
      "Category mtrl-sci:\n",
      "lennard, fracture, phase field, molecular dynamics, colloidal, relaxor, exact exchange, paraelectric, first principles, ff state, spin transfer, nw, elasticity, packing, density functional, ferrite, acid, materials, solids, micromagnetic\n",
      "\n",
      "Category quant-ph:\n",
      "quantum, quantum walks, bell, bohmian, transmon, nitrogen vacancy, optomechanical, photosynthetic, entanglement, exceptional points, quant, fluctuation electromagnetic, bose hubbard, pseudospin symmetry, compton wavelength, quantum annealing, plasma model, toric code, telecom, completely positive\n",
      "\n",
      "Category str-el:\n",
      "dynamical mean field, test our theory, falicov, kondo, hidden order, quantum monte, quantum critical, hubbard, electron glasses, holstein, wigner crystal, heavy fermion, numerical renormalization group, 5f, double exchange, manganites, itinerant, moore read, luttinger, luttinger liquid\n",
      "\n",
      "Category supr-con:\n",
      "superconductor, superconductors, superconducting, mgb2, superconductivity, andreev, josephson, htsc, yba2cu3o7, mkids, pnictides, superconductive, bafe2as2, yba2cu3o6, ginzburg landau, critical current, feas, pinning, peak effect, cacuo2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "most_important_words = clf_supervised.named_steps['clf'].coef_.argsort()[:, ::-1]\n",
    "\n",
    "terms =  clf_supervised.named_steps['vect'].get_feature_names()\n",
    "for i in range(len(category_list)):\n",
    "    print \"Category %s:\" % (category_list[i])\n",
    "    print ', '.join([terms[x] for x in most_important_words[i, :20]])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try KMeans clustering. \n",
    "See: http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10316.5555911\n",
      "127.936686993\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "# Reduce n_init to 10 for testing purposes.\n",
    "clf_unsupervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3), stop_words='english')),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', KMeans(n_clusters=n_clusters, n_init=10))])\n",
    "start = time.time()\n",
    "clf_unsupervised.fit(x_train)\n",
    "print time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "predict_train = clf_unsupervised.predict(x_train)\n",
    "predict = clf_unsupervised.predict(x_test)\n",
    "print time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which clusters most closely align with which the original categories?\n",
    "# Find the strongest correlation, and assign that cluster to the category. Iterate.\n",
    "# matrix_train = [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))\n",
    "#                    for x in range(0,n_clusters)] \n",
    "#                    for cat in category_list]\n",
    "\n",
    "\n",
    "counter_category = Counter(y_train)\n",
    "counter_cluster = Counter(predict_train)\n",
    "accuracy_train_initial = np.array(\n",
    "#     [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "    [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_category[cat]\n",
    "           for x in range(0,n_clusters)] \n",
    "           for cat in category_list])\n",
    "clusterToCategory = dict()\n",
    "# categoryToCluster = dict()\n",
    "\n",
    "for cluster, item in enumerate(np.argmax(accuracy_train_initial, axis=0).tolist()):\n",
    "    clusterToCategory[cluster] = category_list[item]\n",
    "\n",
    "\n",
    "# category_list_remaining = list(category_list)\n",
    "# cluster_list_remaining = range(0, n_clusters)\n",
    "# for x in range(0, len(category_list)):\n",
    "#     accuracy_train = np.array(\n",
    "#                         [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "#                            for x in cluster_list_remaining] \n",
    "#                            for cat in category_list_remaining])\n",
    "\n",
    "    \n",
    "#     # Find largest value in the category axis\n",
    "#     cat_ind, cluster_ind = np.unravel_index(np.argmax(accuracy_train), accuracy_train.shape)\n",
    "#     clusterToCategory[cluster_list_remaining[cluster_ind]] = category_list_remaining[cat_ind]\n",
    "#     categoryToCluster[category_list_remaining[cat_ind]] = cluster_list_remaining[cluster_ind]\n",
    "    \n",
    "#     # Remove those entries from the lists.\n",
    "#     category_list_remaining.remove(category_list_remaining[cat_ind])\n",
    "#     cluster_list_remaining.remove(cluster_list_remaining[cluster_ind])\n",
    "# #     break\n",
    "\n",
    "# # The remaining clusters predict empty strings\n",
    "# for item in cluster_list_remaining:\n",
    "#     clusterToCategory[item] = ''\n",
    "# clusterToCategory_list = sorted(clusterToCategory.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "          0         1         2         3         4         5         6         7         8         9         \n",
      "          hep-      hep-      quant-ph  str-el    hep-      hep-      quant-ph  hep-      supr-con  hep-      \n",
      "hep-      0.25      0.037     0.32      0.013     0.12      0.036     0.008     0.17      0.013     0.034     \n",
      "mtrl-sci  0.0017    0.00012   0.28      0.14      0.0       0.00016   0.0057    0.0004    0.57      4e-05     \n",
      "quant-ph  0.01      0.00089   0.38      0.042     7.2e-05   0.00029   0.54      0.00053   0.025     0.00024   \n",
      "str-el    0.012     0.00025   0.18      0.37      0.00012   0.0016    0.029     0.0       0.4       0.0       \n",
      "supr-con  0.0033    9.3e-05   0.13      0.16      9.3e-05   0.0022    0.02      9.3e-05   0.68      0.0       \n"
     ]
    }
   ],
   "source": [
    "# The table is normalized by number of elements in each cluster.\n",
    "print 'Training data'\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "for cat, item in zip(category_list, accuracy_train_initial):\n",
    "    print ('{:<10}' + '{:<10.2}'*n_clusters).format(cat, *item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "          0         1         2         3         4         5         6         7         8         9         \n",
      "          hep-      hep-      quant-ph  str-el    hep-      hep-      quant-ph  hep-      supr-con  hep-      \n",
      "hep-      8083      1243      5824      504       3965      1245      362       5732      479       1176      \n",
      "mtrl-sci  26        0         1042      1032      0         4         51        4         4150      0         \n",
      "quant-ph  186       18        2789      528       5         6         6601      20        387       7         \n",
      "str-el    61        3         458       1845      0         9         182       0         1624      0         \n",
      "supr-con  12        2         170       580       0         6         75        0         1836      1         \n"
     ]
    }
   ],
   "source": [
    "# Is there overlap between the clusters and existing categories ('ground truth')?\n",
    "matrix = [[sum((a==cat and b==x for a,b in zip(y_test, predict)))\n",
    "           for x in range(0,n_clusters)] \n",
    "           for cat in category_list]\n",
    "\n",
    "print 'Test data'\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "for cat, item in zip(category_list, matrix):\n",
    "    print ('{:<10}' + '{:<10}'*n_clusters).format(cat, *item)\n",
    "    \n",
    "    \n",
    "# # Oops, this is the same as the confusion matrix?\n",
    "# tmp_reverse_category = dict([(y,x) for x,y in enumerate(category_list)])\n",
    "# y_test_num = [tmp_reverse_category[x] for x in y_test]\n",
    "# print ''\n",
    "# print 'Confusion matrix:'\n",
    "# print confusion_matrix(y_test_num, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       hep-       0.98      0.75      0.85     28613\n",
      "   mtrl-sci       0.00      0.00      0.00      6309\n",
      "   quant-ph       0.53      0.89      0.67     10547\n",
      "     str-el       0.41      0.44      0.43      4182\n",
      "   supr-con       0.22      0.68      0.33      2682\n",
      "\n",
      "avg / total       0.69      0.66      0.65     52333\n",
      "\n",
      "Accuracy score: 0.66\n",
      "[[21444     0  6186   504   479]\n",
      " [   34     0  1093  1032  4150]\n",
      " [  242     0  9390   528   387]\n",
      " [   73     0   640  1845  1624]\n",
      " [   21     0   245   580  1836]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# We can now make a prediction based on these categories.\n",
    "predict_category = [clusterToCategory[y] for y in predict]\n",
    "\n",
    "print(classification_report(y_test, predict_category))\n",
    "print('Accuracy score: %0.2f' % accuracy_score(y_test, predict_category))\n",
    "print confusion_matrix(y_test, predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (hep-):\n",
      "theory, gauge, field, theories, string, space, dimensional, non, gravity, solutions, brane, action, fields, branes, field theory, conformal, equations, scalar, algebra, model\n",
      "\n",
      "Cluster 1 (hep-):\n",
      "black, black hole, hole, black holes, holes, horizon, solutions, entropy, hawking, gravity, extremal, ads, dimensional, rotating, solution, scalar, field, theory, kerr, schwarzschild\n",
      "\n",
      "Cluster 2 (quant-ph):\n",
      "model, energy, time, results, quantum, method, using, field, equation, non, state, potential, new, present, function, order, theory, particle, study, states\n",
      "\n",
      "Cluster 3 (str-el):\n",
      "spin, magnetic, field, magnetic field, phase, state, temperature, quantum, magnetization, coupling, model, ferromagnetic, electron, order, transition, effect, spin orbit, ground, orbit, states\n",
      "\n",
      "Cluster 4 (hep-):\n",
      "neutrino, higgs, model, mass, standard model, standard, boson, cp, mixing, higgs boson, masses, mu, neutrinos, lepton, tau, decay, gev, models, supersymmetric, flavor\n",
      "\n",
      "Cluster 5 (hep-):\n",
      "pi, pi pi, pm, gamma, decays, decay, psi, branching, bar, eta, cp, 10, pi pi pi, rho, data, stat, detector, collected, syst, meson\n",
      "\n",
      "Cluster 6 (quant-ph):\n",
      "quantum, entanglement, states, state, classical, qubit, information, systems, entangled, photon, measurement, time, qubits, mechanics, single, quantum mechanics, non, protocol, scheme, quantum information\n",
      "\n",
      "Cluster 7 (hep-):\n",
      "quark, production, cross, collisions, qcd, data, heavy, proton, gev, jet, mass, cross section, section, gluon, parton, bar, transverse, results, energy, hadron\n",
      "\n",
      "Cluster 8 (supr-con):\n",
      "temperature, phase, transition, electron, graphene, surface, density, band, electronic, energy, superconducting, high, structure, properties, low, critical, gap, magnetic, field, state\n",
      "\n",
      "Cluster 9 (hep-):\n",
      "dark, dark matter, matter, dm, model, dark energy, annihilation, mass, energy, detection, models, relic, universe, ray, particles, direct, gamma, constraints, direct detection, gev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "order_centroids = clf_unsupervised.named_steps['clf'].cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms =  clf_unsupervised.named_steps['vect'].get_feature_names()\n",
    "for i in range(n_clusters):\n",
    "    print \"Cluster %d (%s):\" % (i, clusterToCategory[i])\n",
    "    print ', '.join([terms[x] for x in order_centroids[i, :20]])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(clf_supervised, 'svm_category.pkl')\n",
    "joblib.dump(clf_unsupervised, 'kmeans_category.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is another interesting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf_pca = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                              ('tfidf', TfidfTransformer()),\n",
    "#                              ('clf', KMeans(n_components=6))])\n",
    "# X = clf_pca.fit(x_train)\n",
    "# # predict = clf_pca.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
