{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fancy machine learning to predict whether an article makes it into Nature/Science or PRL. This time we'll only look at articles in the physics.atom-ph section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import combinations\n",
    "import json\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "# from nltk.corpus import stopwords\n",
    "# s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['atom-ph', 'quant-ph', 'cond-mat', 'quant-gas', 'hep-th', 'hep-ex']\n",
    "# categories = ['atom-ph', 'quant-ph']\n",
    "journals_dict = {'PRL': ['Physics Review Letters%',\n",
    "                    'Phys. Rev. Lett.%',\n",
    "                    'Phys.Rev.Lett.%',\n",
    "                    'PRL%'],\n",
    "                 'PR':  ['Physics Review%',\n",
    "                         'Phys. Rev.%',\n",
    "                         'Phys.Rev.%',\n",
    "                         'PR%'],\n",
    "                 'Nature': ['Nature%',\n",
    "                            'Nat.%',\n",
    "                            'Science%'],\n",
    "                 'APL': ['APL%',\n",
    "                         'Appl.Phys.Lett.%',\n",
    "                         'Appl. Phys. Lett.%',\n",
    "                         'Applied Physics Letters%'],\n",
    "                 'AP': ['AP%',\n",
    "                        'Appl.Phys.%',\n",
    "                        'Appl. Phys.%',\n",
    "                        'Applied Physics%'],\n",
    "                 'PL': ['Physics Letters%',\n",
    "                        'Phys. Lett.%',\n",
    "                        'Phys.Lett.%'],\n",
    "                 'All': ['%'],\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_abstracts(category):\n",
    "    query = session.query(Article_Category)\\\n",
    "                    .join(Category)\\\n",
    "                    .join(Article)\\\n",
    "                    .filter(Category.name.like('%' + category + '%'),\n",
    "                            Article.journal_ref.like('Phys.Rev.Lett%'))\n",
    "                \n",
    "    # Don't need to clean up text: CountVectorizer will do everything\n",
    "    return query\n",
    "#     return [ result.article.abstract for result in query ]\n",
    "\n",
    "\n",
    "# def learn(journal, categories):\n",
    "#     abstracts1 = get_abstracts(journals[0], category)\n",
    "#     abstracts2 = get_abstracts(journals[1], category)\n",
    "    \n",
    "#     half_test_size = int(round(0.2*min(len(abstracts1),len(abstracts2))))\n",
    "# #     print half_test_size\n",
    "\n",
    "#     X1_train, X1_test, y1_train, y1_test = train_test_split(abstracts1, [0]*len(abstracts1), test_size=half_test_size, random_state=42)\n",
    "#     X2_train, X2_test, y2_train, y2_test = train_test_split(abstracts2, [1]*len(abstracts2), test_size=half_test_size, random_state=42)\n",
    "\n",
    "#     X_train = X1_train + X2_train\n",
    "#     X_test = X1_test + X2_test\n",
    "#     y_train = np.array(y1_train + y2_train)\n",
    "#     y_test = np.array(y1_test + y2_test)\n",
    "#     target_names = journals\n",
    "    \n",
    "    \n",
    "#     clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', OneVsRestClassifier(\n",
    "#                                 LinearSVC(C=1,penalty='l1',dual=False,fit_intercept=True)))])\n",
    "#     transform = clf.fit_transform(X_train, y_train)\n",
    "#     y_predict_train = clf.predict(X_train)\n",
    "#     y_predict_test = clf.predict(X_test)\n",
    "    \n",
    "#     X_train_tfidf = clf.named_steps['tfidf'].transform(\n",
    "#                         clf.named_steps['vect'].transform(X_train))\n",
    "    \n",
    "#     print (metrics.classification_report(y_test, y_predict_test,\n",
    "#                                     target_names=target_names))\n",
    "# #                                     target_names=test_target_names))\n",
    "#     print metrics.confusion_matrix(y_test, y_predict_test)\n",
    "#     print 'Accuracy: %f' % (metrics.accuracy_score(y_test, y_predict_test))\n",
    "\n",
    "# #     most_important_words = clf.named_steps['clf'].coef_.argsort()[:, ::-1]\n",
    "\n",
    "# #     print np.squeeze(X_train_tfidf[y_train==0, 3].toarray()).shape\n",
    "# #     print np.squeeze(X_train_tfidf[y_train==0, 3].toarray()).shape\n",
    "    \n",
    "    \n",
    "# #     terms =  clf.named_steps['vect'].get_feature_names()\n",
    "# #     result = [{'name': terms[word],\n",
    "# #                'value': clf.named_steps['clf'].coef_[0,word],\n",
    "# #                'vector1': np.squeeze(X_train_tfidf[y_train==0, word].toarray()).tolist(),\n",
    "# #                'vector2': np.squeeze(X_train_tfidf[y_train==1, word].toarray()).tolist(),\n",
    "# #               }\n",
    "# #                 for word in (np.concatenate((most_important_words[0, :15],\n",
    "# #                              most_important_words[0, -15:]))) ]\n",
    "#     return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some abstract have multiple categories. I'll make a dict based on the article\n",
    "# id number to link these labels.\n",
    "query_list = []\n",
    "for category in categories:\n",
    "    query_list.append(get_abstracts(category))\n",
    "    \n",
    "abstract_dict = dict()\n",
    "category_dict = dict()\n",
    "category_to_number = dict(zip(categories, range(0, len(categories))))\n",
    "\n",
    "\n",
    "for q, category in zip(query_list, categories):\n",
    "    for x in q:\n",
    "        abstract_dict[x.article.id] = x.article.abstract\n",
    "        try:\n",
    "            category_dict[x.article.id].append(category)\n",
    "        except KeyError:\n",
    "            category_dict[x.article.id] = [category]\n",
    "keys = abstract_dict.keys()\n",
    "\n",
    "X_train = [abstract_dict[key] for key in keys]\n",
    "Y_train = [ category_dict[key] for key in keys]\n",
    "# Y_train = [ [category_to_number[x] for x in category_dict[key]] for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.py:194: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.py:194: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.py:194: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n",
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.py:194: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
       "        charset_error=None, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), prep...ling=1, loss='l2', multi_class='ovr', penalty='l1',\n",
       "     random_state=None, tol=0.0001, verbose=0))]),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_category = OneVsRestClassifier(Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(C=1,penalty='l1',dual=False,fit_intercept=True))]))\n",
    "clf_category.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py:636: DeprecationWarning: Direct support for sequence of sequences multilabel representation will be unavailable from version 0.17. Use sklearn.preprocessing.MultiLabelBinarizer to convert to a label indicator representation.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cond-mat', 'hep-ex')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_category.predict(['We measure the mass, gap, and magnetic moment of a magnon in the ferromagnetic F=1 spinor Bose-Einstein condensate of 87Rb. We find an unusually heavy magnon mass of 1.038(2)stat(8)sys times the atomic mass, as determined by interfering standing and running coherent magnon waves within the dense and trapped condensed gas. This measurement is shifted significantly from theoretical estimates. The magnon energy gap of h×2.5(1)stat(2)sysHz and the effective magnetic moment of −1.04(2)stat(8)μbare times the atomic magnetic moment are consistent with mean-field predictions. The nonzero energy gap arises from magnetic dipole-dipole interactions.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print metrics.classification_report(Y_train, clf.predict(X_train))\n",
    "#                                     target_names=test_target_names))\n",
    "# print metrics.confusion_matrix(y_test, y_predict_test)\n",
    "# print 'Accuracy: %f' % (metrics.accuracy_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_category.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_category, 'svm_category.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
