{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I predict the existence of subfields with some cool unsupervised learning algorithm? \n",
    "\n",
    "For starters, let's just use regular n-grams. A more advanced version would be to look for noun phrases or J&K POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "%matplotlib inline\n",
    "# import matplotlib.pyplot as plt \n",
    "import time\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "# from nltk.corpus import stopwords\n",
    "# s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn import metrics\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'acc-phys', u'adap-org', u'alg-geom', u'ao-sci', u'astro-ph', u'astro-ph.CO', u'astro-ph.EP', u'astro-ph.GA', u'astro-ph.HE', u'astro-ph.IM', u'astro-ph.SR', u'atom-ph', u'bayes-an', u'chao-dyn', u'chem-ph', u'comp-gas', u'cond-mat', u'cond-mat.dis-nn', u'cond-mat.mes-hall', u'cond-mat.mtrl-sci', u'cond-mat.other', u'cond-mat.quant-gas', u'cond-mat.soft', u'cond-mat.stat-mech', u'cond-mat.str-el', u'cond-mat.supr-con', u'cs.AI', u'cs.AR', u'cs.CC', u'cs.CE', u'cs.CG', u'cs.CL', u'cs.CR', u'cs.CV', u'cs.CY', u'cs.DB', u'cs.DC', u'cs.DL', u'cs.DM', u'cs.DS', u'cs.ET', u'cs.FL', u'cs.GL', u'cs.GR', u'cs.GT', u'cs.HC', u'cs.IR', u'cs.IT', u'cs.LG', u'cs.LO', u'cs.MA', u'cs.MM', u'cs.MS', u'cs.NA', u'cs.NE', u'cs.NI', u'cs.OH', u'cs.PF', u'cs.PL', u'cs.RO', u'cs.SC', u'cs.SD', u'cs.SE', u'cs.SI', u'cs.SY', u'dg-ga', u'funct-an', u'gr-qc', u'hep-ex', u'hep-lat', u'hep-ph', u'hep-th', u'math-ph', u'math.AC', u'math.AG', u'math.AP', u'math.AT', u'math.CA', u'math.CO', u'math.CT', u'math.CV', u'math.DG', u'math.DS', u'math.FA', u'math.GM', u'math.GN', u'math.GR', u'math.GT', u'math.HO', u'math.IT', u'math.KT', u'math.LO', u'math.MG', u'math.MP', u'math.NA', u'math.NT', u'math.OA', u'math.OC', u'math.PR', u'math.QA', u'math.RA', u'math.RT', u'math.SG', u'math.SP', u'math.ST', u'mtrl-th', u'nlin.AO', u'nlin.CD', u'nlin.CG', u'nlin.PS', u'nlin.SI', u'nucl-ex', u'nucl-th', u'patt-sol', u'physics.acc-ph', u'physics.ao-ph', u'physics.atm-clus', u'physics.atom-ph', u'physics.bio-ph', u'physics.chem-ph', u'physics.class-ph', u'physics.comp-ph', u'physics.data-an', u'physics.ed-ph', u'physics.flu-dyn', u'physics.gen-ph', u'physics.geo-ph', u'physics.hist-ph', u'physics.ins-det', u'physics.med-ph', u'physics.optics', u'physics.plasm-ph', u'physics.pop-ph', u'physics.soc-ph', u'physics.space-ph', u'plasm-ph', u'q-alg', u'q-bio', u'q-bio.BM', u'q-bio.CB', u'q-bio.GN', u'q-bio.MN', u'q-bio.NC', u'q-bio.OT', u'q-bio.PE', u'q-bio.QM', u'q-bio.SC', u'q-bio.TO', u'q-fin.CP', u'q-fin.EC', u'q-fin.GN', u'q-fin.MF', u'q-fin.PM', u'q-fin.PR', u'q-fin.RM', u'q-fin.ST', u'q-fin.TR', u'quant-ph', u'solv-int', u'stat.AP', u'stat.CO', u'stat.ME', u'stat.ML', u'stat.OT', u'stat.TH', u'supr-con']\n"
     ]
    }
   ],
   "source": [
    "# What are the available categories?\n",
    "categories = sorted([x.name for x in session.query(Category)])\n",
    "print categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.8727078438\n"
     ]
    }
   ],
   "source": [
    "abstract_all_tmp = {'category': [], 'abstract': []}\n",
    "category_list = sorted(['atom-ph', 'quant-ph', 'optics', 'nlin', 'str-el', 'stat'])\n",
    "# category_list = sorted(['quant-ph', 'str-el', 'hep-', 'mtrl-sci', 'supr-con'])\n",
    "category_len = len(category_list)\n",
    "\n",
    "start = time.time()\n",
    "for item in category_list:\n",
    "    query = session.query(Article_Category)\\\n",
    "                        .join(Category)\\\n",
    "                        .join(Article)\\\n",
    "                        .filter(Category.name.like('%' + item + '%'),\n",
    "                                or_(Article.journal_ref.like('Physics Review Letters%'),\n",
    "                                              Article.journal_ref.like('Phys. Rev. Lett.%'),\n",
    "                                              Article.journal_ref.like('PRL%')))\n",
    "#     query = session.query(Article_Category)\\\n",
    "#                         .join(Category)\\\n",
    "#                         .join(Article)\\\n",
    "#                         .filter(Category.name.like('%' + item + '%'))\n",
    "    result = [' '.join(x.article.abstract.split()) for x in query]\n",
    "    abstract_all_tmp['abstract'].extend(result)\n",
    "    abstract_all_tmp['category'].extend([item]*len(result))\n",
    "print time.time() - start\n",
    "# for item in query:\n",
    "#     abstract_all['category'].append(item.category.name)\n",
    "#     abstract_all['abstract'].append(' '.join(item.article.abstract.split()))\n",
    "# print time.time() - start\n",
    "# abstract_all['atom-ph'] = [x.article.abstract for x in query.all()]\n",
    "# session.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat           1850\n",
      "atom-ph        575\n",
      "str-el         3269\n",
      "nlin           384\n",
      "optics         504\n",
      "quant-ph       3092\n",
      "Total          9674\n"
     ]
    }
   ],
   "source": [
    "# Breakdown of categories?\n",
    "count = Counter(abstract_all_tmp['category'])\n",
    "for key, val in count.iteritems():\n",
    "    print '{:<15}{}'.format(key, val)\n",
    "print '{:<15}{}'.format('Total', len(abstract_all_tmp['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7070\n",
      "7070\n"
     ]
    }
   ],
   "source": [
    "##Oops! How many overlapping articles do we have? I forgot that arXiv categories aren't unique.\n",
    "# Let's remove all duplicates.\n",
    "# This is slow but I am tired.\n",
    "\n",
    "counter_duplicate = Counter(abstract_all_tmp['abstract'])\n",
    "\n",
    "abstract_all = {'category': [], 'abstract': []}\n",
    "for cat, abstract in itertools.izip(abstract_all_tmp['category'], abstract_all_tmp['abstract']):\n",
    "    if counter_duplicate[abstract] == 1:\n",
    "        abstract_all['category'].append(cat)\n",
    "        abstract_all['abstract'].append(abstract)\n",
    "print len(abstract_all['category'])\n",
    "print len(abstract_all['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat           1301\n",
      "atom-ph        268\n",
      "str-el         2846\n",
      "nlin           191\n",
      "optics         242\n",
      "quant-ph       2222\n",
      "Total          7070\n"
     ]
    }
   ],
   "source": [
    "# Breakdown of categories? That's a lot of repetition!!!\n",
    "count = Counter(abstract_all['category'])\n",
    "for key, val in count.iteritems():\n",
    "    print '{:<15}{}'.format(key, val)\n",
    "print '{:<15}{}'.format('Total', len(abstract_all['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train on 80% of the data. Random_state ensures that we always get the same result.\n",
    "x_train, x_test, y_train, y_test = train_test_split(abstract_all['abstract'],\n",
    "                                                    abstract_all['category'],\n",
    "                                                    random_state=42,\n",
    "                                                    train_size=0.8)\n",
    "\n",
    "counter_train = Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I lied, I'm starting with supervised learning (as a comparison). We're looking at ~60-70% accuracy for these cateogories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.4500420094\n",
      "1.16515302658\n"
     ]
    }
   ],
   "source": [
    "#SVC(kernel='linear') is good\n",
    "clf_supervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "#                            ('clf', LinearSVC())])\n",
    "                           ('clf', LinearSVC(C=1,penalty='l1',dual=False,))])\n",
    "start = time.time()\n",
    "clf_supervised.fit(x_train, y_train)\n",
    "print time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "predict = clf_supervised.predict(x_test)\n",
    "print time.time() - start\n",
    "#print text_abstract_clf.predict(train_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    atom-ph       0.76      0.58      0.66        60\n",
      "       nlin       0.71      0.23      0.35        43\n",
      "     optics       0.78      0.62      0.69        50\n",
      "   quant-ph       0.88      0.89      0.88       448\n",
      "       stat       0.78      0.80      0.79       266\n",
      "     str-el       0.87      0.94      0.90       547\n",
      "\n",
      "avg / total       0.84      0.85      0.84      1414\n",
      "\n",
      "Accuracy score: 0.85\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict))\n",
    "print('Accuracy score: %0.2f' % accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category atom-ph:\n",
      "ionization, the fine structure, collisions, 2s, molecules, limits on, ultracold, clock, positron, laser, atoms, the factor, feshbach, attosecond, atom, dipole, edm, helium, precision, 10\n",
      "\n",
      "Category nlin:\n",
      "chaotic, turbulence, patterns, random matrix, oscillators, periodic, nonlinear, billiards, orbits, flow, numerical, turbulent, flows, solution, kicked, semiclassical, solitons, synchronization, structure functions, frequencies\n",
      "\n",
      "Category optics:\n",
      "photonic, plasmonic, metamaterial, metamaterials, radiation, optical, media, polariton, electromagnetic, lasing, optically, light, resonators, transmission, beam, plasmon, pulses, material, generation, wavelength\n",
      "\n",
      "Category quant-ph:\n",
      "quantum, qubit, qubits, entanglement, bell, detuning, entangled, scheme, casimir, photon, photons, operators, measurement, cavity, optimal, inequality, variable, nitrogen, detection, vacuum\n",
      "\n",
      "Category stat:\n",
      "simulations, granular, stochastic, hard, dna, growth, random, equilibrium, thermodynamics, percolation, fluctuation, hydrodynamic, molecular, free energy, law, glass, length, colloidal, packing, and of\n",
      "\n",
      "Category str-el:\n",
      "quantum critical, kondo, antiferromagnetic, neutron, coulomb, renormalization, fermi, itinerant, fermion, magnetic, dynamical mean, metal, electronic, luttinger, lattice, hubbard, superconductors, superconductivity, excitations, heisenberg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "most_important_words = clf_supervised.named_steps['clf'].coef_.argsort()[:, ::-1]\n",
    "\n",
    "terms =  clf_supervised.named_steps['vect'].get_feature_names()\n",
    "for i in range(len(category_list)):\n",
    "    print \"Category %s:\" % (category_list[i])\n",
    "    print ', '.join([terms[x] for x in most_important_words[i, :20]])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try KMeans clustering. \n",
    "See: http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446.83970499\n",
      "3.38381505013\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "# Reduce n_init to 10 for testing purposes.\n",
    "clf_unsupervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3), stop_words='english')),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', KMeans(n_clusters=n_clusters, n_init=50))])\n",
    "start = time.time()\n",
    "clf_unsupervised.fit(x_train)\n",
    "print time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "predict_train = clf_unsupervised.predict(x_train)\n",
    "predict = clf_unsupervised.predict(x_test)\n",
    "print time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which clusters most closely align with which the original categories?\n",
    "# Find the strongest correlation, and assign that cluster to the category. Iterate.\n",
    "# matrix_train = [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))\n",
    "#                    for x in range(0,n_clusters)] \n",
    "#                    for cat in category_list]\n",
    "\n",
    "\n",
    "counter_category = Counter(y_train)\n",
    "counter_cluster = Counter(predict_train)\n",
    "accuracy_train_initial = np.array(\n",
    "#     [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "    [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_category[cat]\n",
    "           for x in range(0,n_clusters)] \n",
    "           for cat in category_list])\n",
    "clusterToCategory = dict()\n",
    "# categoryToCluster = dict()\n",
    "\n",
    "for cluster, item in enumerate(np.argmax(accuracy_train_initial, axis=0).tolist()):\n",
    "    clusterToCategory[cluster] = category_list[item]\n",
    "\n",
    "\n",
    "# category_list_remaining = list(category_list)\n",
    "# cluster_list_remaining = range(0, n_clusters)\n",
    "# for x in range(0, len(category_list)):\n",
    "#     accuracy_train = np.array(\n",
    "#                         [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "#                            for x in cluster_list_remaining] \n",
    "#                            for cat in category_list_remaining])\n",
    "\n",
    "    \n",
    "#     # Find largest value in the category axis\n",
    "#     cat_ind, cluster_ind = np.unravel_index(np.argmax(accuracy_train), accuracy_train.shape)\n",
    "#     clusterToCategory[cluster_list_remaining[cluster_ind]] = category_list_remaining[cat_ind]\n",
    "#     categoryToCluster[category_list_remaining[cat_ind]] = cluster_list_remaining[cluster_ind]\n",
    "    \n",
    "#     # Remove those entries from the lists.\n",
    "#     category_list_remaining.remove(category_list_remaining[cat_ind])\n",
    "#     cluster_list_remaining.remove(cluster_list_remaining[cluster_ind])\n",
    "# #     break\n",
    "\n",
    "# # The remaining clusters predict empty strings\n",
    "# for item in cluster_list_remaining:\n",
    "#     clusterToCategory[item] = ''\n",
    "# clusterToCategory_list = sorted(clusterToCategory.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "          0         1         2         3         4         5         6         7         8         9         \n",
      "          str-el    atom-ph   nlin      atom-ph   str-el    quant-ph  str-el    str-el    quant-ph  stat      \n",
      "atom-ph   0.0       0.72      0.067     0.053     0.048     0.0       0.0048    0.072     0.0       0.034     \n",
      "nlin      0.0068    0.11      0.76      0.0068    0.014     0.0       0.0068    0.02      0.034     0.047     \n",
      "optics    0.083     0.69      0.13      0.016     0.026     0.0       0.0052    0.026     0.0052    0.016     \n",
      "quant-ph  0.025     0.24      0.067     0.028     0.055     0.16      0.0051    0.0051    0.39      0.029     \n",
      "stat      0.011     0.032     0.61      0.052     0.021     0.0048    0.014     0.016     0.0077    0.23      \n",
      "str-el    0.12      0.014     0.036     0.025     0.099     0.0052    0.15      0.33      0.0026    0.22      \n"
     ]
    }
   ],
   "source": [
    "# The table is normalized by number of elements in each cluster.\n",
    "print 'Training data'\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "for cat, item in zip(category_list, accuracy_train_initial):\n",
    "    print ('{:<10}' + '{:<10.2}'*n_clusters).format(cat, *item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "          0         1         2         3         4         5         6         7         8         9         \n",
      "          str-el    atom-ph   nlin      atom-ph   str-el    quant-ph  str-el    str-el    quant-ph  stat      \n",
      "atom-ph   0         47        3         1         6         0         0         3         0         0         \n",
      "nlin      1         5         26        0         2         0         0         4         2         3         \n",
      "optics    1         38        7         1         1         0         0         0         1         1         \n",
      "quant-ph  12        113       15        9         22        68        1         4         189       15        \n",
      "stat      2         12        146       10        5         0         3         5         5         78        \n",
      "str-el    50        5         27        20        69        1         69        191       2         113       \n"
     ]
    }
   ],
   "source": [
    "# Is there overlap between the clusters and existing categories ('ground truth')?\n",
    "matrix = [[sum((a==cat and b==x for a,b in zip(y_test, predict)))\n",
    "           for x in range(0,n_clusters)] \n",
    "           for cat in category_list]\n",
    "\n",
    "print 'Test data'\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "for cat, item in zip(category_list, matrix):\n",
    "    print ('{:<10}' + '{:<10}'*n_clusters).format(cat, *item)\n",
    "    \n",
    "    \n",
    "# # Oops, this is the same as the confusion matrix?\n",
    "# tmp_reverse_category = dict([(y,x) for x,y in enumerate(category_list)])\n",
    "# y_test_num = [tmp_reverse_category[x] for x in y_test]\n",
    "# print ''\n",
    "# print 'Confusion matrix:'\n",
    "# print confusion_matrix(y_test_num, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    atom-ph       0.18      0.80      0.30        60\n",
      "       nlin       0.12      0.60      0.19        43\n",
      "     optics       0.00      0.00      0.00        50\n",
      "   quant-ph       0.96      0.57      0.72       448\n",
      "       stat       0.37      0.29      0.33       266\n",
      "     str-el       0.84      0.69      0.76       547\n",
      "\n",
      "avg / total       0.71      0.56      0.60      1414\n",
      "\n",
      "Accuracy score: 0.56\n",
      "[[ 48   3   0   0   0   9]\n",
      " [  5  26   0   2   3   7]\n",
      " [ 39   7   0   1   1   2]\n",
      " [122  15   0 257  15  39]\n",
      " [ 22 146   0   5  78  15]\n",
      " [ 25  27   0   3 113 379]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# We can now make a prediction based on these categories.\n",
    "predict_category = [clusterToCategory[y] for y in predict]\n",
    "\n",
    "print(classification_report(y_test, predict_category))\n",
    "print('Accuracy score: %0.2f' % accuracy_score(y_test, predict_category))\n",
    "print confusion_matrix(y_test, predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (str-el):\n",
      "topological, hall, states, quantum hall, quantum, edge, fractional, nu, state, spin, phase, fractional quantum, fractional quantum hall, symmetry, majorana, surface, phases, abelian, chiral, insulator\n",
      "\n",
      "Cluster 1 (atom-ph):\n",
      "photon, optical, atoms, cavity, laser, quantum, light, single, state, field, frequency, atom, atomic, states, pulse, coupling, photons, pulses, time, using\n",
      "\n",
      "Cluster 2 (nlin):\n",
      "time, dynamics, model, distribution, systems, networks, non, random, particles, equilibrium, energy, study, particle, results, network, density, scaling, simulations, theory, law\n",
      "\n",
      "Cluster 3 (atom-ph):\n",
      "phys rev, rev, phys, lett, phys rev lett, rev lett, et al, et, al, comment, al phys, al phys rev, et al phys, reply, cond, mat, cond mat, arxiv, reply comment, recent\n",
      "\n",
      "Cluster 4 (str-el):\n",
      "spin, spin orbit, orbit, coupling, magnetic, quantum, state, electron, excitations, interaction, spins, nuclear, liquid, polarization, exchange, field, model, lattice, interactions, spin liquid\n",
      "\n",
      "Cluster 5 (quant-ph):\n",
      "entanglement, states, entangled, quantum, state, photon, entangled states, information, multipartite, systems, qubit, local, entangled state, entropy, quantum information, bipartite, bound, qubits, bell, arbitrary\n",
      "\n",
      "Cluster 6 (str-el):\n",
      "mott, insulator, transition, metal, mean field, field, mean, hubbard, dynamical mean field, dynamical mean, mean field theory, theory, phase, field theory, dynamical, metal insulator, model, hubbard model, insulator transition, metal insulator transition\n",
      "\n",
      "Cluster 7 (str-el):\n",
      "magnetic, spin, _2, temperature, scattering, field, electron, superconducting, state, order, fermi, structure, energy, high, low, gap, observed, surface, wave, phase\n",
      "\n",
      "Cluster 8 (quant-ph):\n",
      "quantum, qubit, classical, measurement, state, information, states, qubits, measurements, bell, noise, correlations, computation, non, time, systems, error, using, quantum information, single\n",
      "\n",
      "Cluster 9 (stat):\n",
      "phase, temperature, critical, transition, quantum, model, order, kondo, point, density, liquid, spin, disorder, field, diagram, phase diagram, finite, low, lattice, phase transition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "order_centroids = clf_unsupervised.named_steps['clf'].cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms =  clf_unsupervised.named_steps['vect'].get_feature_names()\n",
    "for i in range(n_clusters):\n",
    "    print \"Cluster %d (%s):\" % (i, clusterToCategory[i])\n",
    "    print ', '.join([terms[x] for x in order_centroids[i, :20]])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is another interesting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf_pca = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                              ('tfidf', TfidfTransformer()),\n",
    "#                              ('clf', KMeans(n_components=6))])\n",
    "# X = clf_pca.fit(x_train)\n",
    "# # predict = clf_pca.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
