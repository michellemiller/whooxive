{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import operator\n",
    "import random as rnd\n",
    "\n",
    "from time import time\n",
    "\n",
    "s=stopwords.words('english') + ['']\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()\n",
    "\n",
    "category_name = \"cond-mat\"\n",
    "\n",
    "#ML\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are pretty popular:\n",
    "journals = [[\"Phys Rev Lett\", \"Physical Review Letters\",\"PRL\", \"Phys. Rev. Lett.\"], \\\n",
    "            [\"Phys Rev B\",\"Physical Review B\", \"PRB\", \"Phys. Rev. B\"], \\\n",
    "            [\"Nature\",\"Nat\", \"Nat.\",\"Science\"]]\n",
    "            \n",
    "result_by_journal=[]\n",
    "for idx,j_list in enumerate(journals):\n",
    "    if type(j_list) == str:\n",
    "        query = session.query(Article_Category)\\\n",
    "                .join(Category).join(Article)\\\n",
    "                .filter(Category.name.like('%'+category_name+'%'))\\\n",
    "                .filter(Article.journal_ref.like(\"%\"+j_list+\"%\"))\n",
    "        result_by_journal.insert(idx,query.all())\n",
    "    elif j_list == None:\n",
    "        query = session.query(Article_Category)\\\n",
    "                .join(Category).join(Article)\\\n",
    "                .filter(Category.name.like('%'+category_name+'%'))\\\n",
    "                .filter(Article.journal_ref==None)\n",
    "        result_by_journal.insert(idx,query.all())\n",
    "    else:\n",
    "        for k in range(len(j_list)):\n",
    "            query = session.query(Article_Category)\\\n",
    "                .join(Category).join(Article)\\\n",
    "                .filter(Category.name.like('%'+category_name+'%'))\\\n",
    "                .filter(Article.journal_ref.like(\"%\"+j_list[k]+\"%\"))\n",
    "            if k==0:\n",
    "                result_by_journal.insert(idx,query.all())\n",
    "            else:\n",
    "                result_by_journal[idx]+=query.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make training set including **set_size** examples per journal.\n",
    "def make_training_set(set_size, journal_name, result_by_journal):\n",
    "    '''Takes the size of the training set desired, journal name for one vs all classifier, \n",
    "        and list of all returns from query.\n",
    "        journal_name: 0 == PRL; 1==PRB; 2==Nature etc.'''\n",
    "\n",
    "    PRL_len = len(result_by_journal[0])\n",
    "    PRB_len = len(result_by_journal[1])\n",
    "    Nat_len = len(result_by_journal[2])\n",
    "\n",
    "    if set_size > min([PRL_len,PRB_len,Nat_len]):\n",
    "        print(\"This is going to fail: set size must be less than \"+str(min([PRL_len,PRB_len,Nat_len])))\n",
    "    \n",
    "    #Randomize articles selected to prevent order bias.\n",
    "    random_prl = rnd.sample(xrange(0,PRL_len),set_size)\n",
    "    random_prb = rnd.sample(xrange(0,PRB_len),set_size)\n",
    "    random_nat = rnd.sample(xrange(0,Nat_len),set_size)\n",
    "\n",
    "    abstracts = []\n",
    "    #pulls abstracts (by journal) from all items returned by query.\n",
    "    for idx in range(len(result_by_journal)):\n",
    "        j_abs=[]\n",
    "        for jidx, item in enumerate(result_by_journal[idx]):\n",
    "            j_abs.insert(jidx,item.article.abstract)\n",
    "        abstracts.insert(idx,j_abs)\n",
    "\n",
    "    #picks out abstracts by random index from each journal.  Inserts into total train list in journal order.\n",
    "    test_corpus = []\n",
    "    for idx in random_prl:\n",
    "        test_corpus.insert(len(test_corpus),abstracts[0][idx])\n",
    "    for idx in random_prb:\n",
    "        test_corpus.insert(len(test_corpus),abstracts[1][idx])\n",
    "    for idx in random_nat:\n",
    "        test_corpus.insert(len(test_corpus),abstracts[2][idx])\n",
    "    \n",
    "    y_vals = zeros(set_size*3)\n",
    "    if journal_name is 0:\n",
    "        y_vals[:set_size] = 1\n",
    "    elif journal_name is 1:\n",
    "        y_vals[set_size:2*set_size] = 1\n",
    "    elif journal_name is 2:\n",
    "        y_vals[2*set_size:] = 1\n",
    "        \n",
    "    return test_corpus, y_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "   'clf__C': np.logspace(-3,3,7)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'tfidf', 'clf'])\n",
      "parameters:\n",
      "{'clf__C': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "done in 77.252s\n",
      "Value of clf_C: 0.001000 \tScore: 0.667\n",
      "Value of clf_C: 0.010000 \tScore: 0.667\n",
      "Value of clf_C: 0.100000 \tScore: 0.668\n",
      "Value of clf_C: 1.000000 \tScore: 0.731\n",
      "Value of clf_C: 10.000000 \tScore: 0.763\n",
      "Value of clf_C: 100.000000 \tScore: 0.762\n",
      "Value of clf_C: 1000.000000 \tScore: 0.761\n",
      "Best score: 0.763\n",
      "Best parameters set:\n",
      "\tclf__C: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "#Does using 2-grams improve results?  Let's investigate for all journals...\n",
    "\n",
    "j_val = 0 #PRL\n",
    "set_size = 8000 #number of articles taken from each of three journals.\n",
    "\n",
    "grid_search_prl = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t_corp, y_vals = make_training_set(set_size, j_val, result_by_journal)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search_prl.fit(t_corp, y_vals)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "all_params = grid_search_prl.grid_scores_\n",
    "for item in all_params:\n",
    "    print(\"Value of clf_C: %f \\tScore: %0.3f\" % (item[0]['clf__C'],item[1]))\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search_prl.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_prl.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'tfidf', 'clf'])\n",
      "parameters:\n",
      "{'clf__C': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "done in 76.668s\n",
      "Value of clf_C: 0.001000 \tScore: 0.667\n",
      "Value of clf_C: 0.010000 \tScore: 0.667\n",
      "Value of clf_C: 0.100000 \tScore: 0.745\n",
      "Value of clf_C: 1.000000 \tScore: 0.872\n",
      "Value of clf_C: 10.000000 \tScore: 0.910\n",
      "Value of clf_C: 100.000000 \tScore: 0.910\n",
      "Value of clf_C: 1000.000000 \tScore: 0.909\n",
      "Best score: 0.910\n",
      "Best parameters set:\n",
      "\tclf__C: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "#Does using 2-grams improve results?  Let's investigate for all journals...\n",
    "\n",
    "j_val = 2 #Nat. etc.\n",
    "set_size = 8000 #number of articles taken from each of three journals.\n",
    "\n",
    "grid_search_nat = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t_corp, y_vals = make_training_set(set_size, j_val, result_by_journal)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search_nat.fit(t_corp, y_vals)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "all_params = grid_search_nat.grid_scores_\n",
    "for item in all_params:\n",
    "    print(\"Value of clf_C: %f \\tScore: %0.3f\" % (item[0]['clf__C'],item[1]))\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search_nat.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_nat.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'tfidf', 'clf'])\n",
      "parameters:\n",
      "{'clf__C': array([  1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
      "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
      "         1.00000000e+03])}\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
      "done in 77.122s\n",
      "Value of clf_C: 0.001000 \tScore: 0.667\n",
      "Value of clf_C: 0.010000 \tScore: 0.667\n",
      "Value of clf_C: 0.100000 \tScore: 0.696\n",
      "Value of clf_C: 1.000000 \tScore: 0.760\n",
      "Value of clf_C: 10.000000 \tScore: 0.777\n",
      "Value of clf_C: 100.000000 \tScore: 0.773\n",
      "Value of clf_C: 1000.000000 \tScore: 0.771\n",
      "Best score: 0.777\n",
      "Best parameters set:\n",
      "\tclf__C: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "#Does using 2-grams improve results?  Let's investigate for all journals...\n",
    "\n",
    "j_val = 1 #PRB\n",
    "set_size = 8000 #number of articles taken from each of three journals.\n",
    "\n",
    "grid_search_prb = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t_corp, y_vals = make_training_set(set_size, j_val, result_by_journal)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search_prb.fit(t_corp, y_vals)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "all_params = grid_search_prb.grid_scores_\n",
    "for item in all_params:\n",
    "    print(\"Value of clf_C: %f \\tScore: %0.3f\" % (item[0]['clf__C'],item[1]))\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search_prb.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search_prb.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PRL Test (cond-mat)\n",
    "j_val = 0\n",
    "n_trainsets = 3\n",
    "\n",
    "#Turn verbosity off\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "\n",
    "#Test the quality of the fit vs training set size.\n",
    "set_sizes=[50, 200, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "\n",
    "#Record as fcn of set_size\n",
    "clfC = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "best_score = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "\n",
    "for ss_idx, ss in enumerate(set_sizes):\n",
    "    for k in range(n_trainsets):\n",
    "        t_corp, y_vals = make_training_set(ss, j_val, result_by_journal)\n",
    "        grid_search.fit(t_corp, y_vals)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        best_score[k][ss_idx] = grid_search.best_score_\n",
    "        clfC[k][ss_idx] = best_parameters['clf__C']\n",
    "\n",
    "        #print(\"Best score for size %g: %0.3f\" % (ss, grid_search.best_score_)\n",
    "\n",
    "\n",
    "#Plotting:        \n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, best_score[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Best fit score')\n",
    "title('PRL: Performance of best fit (four iterations)')\n",
    "show()\n",
    "yscale('log')\n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, clfC[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Regularization parameter')\n",
    "title('PRL: Regularization of best fit (four iterations)')\n",
    "show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'tfidf', 'clf'])\n",
      "parameters:\n",
      "{'clf__C': array([  1.00000000e-02,   4.64158883e-01,   2.15443469e+01,\n",
      "         1.00000000e+03])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-339bf57fa8f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mt_corp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_by_journal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_corp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;31m# to the caller instead of returning any result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                         \u001b[0;31m# In case we had to terminate a managed pool, let\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'multiprocessing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/multiprocessing/util.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    206\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining task handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining result handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.join(): timed out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michelle/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0m_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgotit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#PRB Test (cond-mat)\n",
    "j_val = 1\n",
    "n_trainsets = 4\n",
    "\n",
    "#Turn verbosity off\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "#Test the quality of the fit vs training set size.\n",
    "set_sizes=[50, 200, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "\n",
    "#Record as fcn of set_size\n",
    "clfC = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "best_score = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "\n",
    "for ss_idx, ss in enumerate(set_sizes):\n",
    "    for k in range(n_trainsets):\n",
    "        t_corp, y_vals = make_training_set(ss, j_val, result_by_journal)\n",
    "        grid_search.fit(t_corp, y_vals)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        best_score[k][ss_idx] = grid_search.best_score_\n",
    "        clfC[k][ss_idx] = best_parameters['clf__C']\n",
    "\n",
    "        #print(\"Best score for size %g: %0.3f\" % (ss, grid_search.best_score_)\n",
    "\n",
    "#Plotting\n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, best_score[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Best fit score')\n",
    "title('PRB: Performance of best fit (four iterations)')\n",
    "show()\n",
    "yscale('log')\n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, clfC[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Regularization parameter')\n",
    "title('PRB: Regularization of best fit (four iterations)')\n",
    "show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nat Test (cond-mat)\n",
    "j_val = 2\n",
    "n_trainsets = 4\n",
    "\n",
    "#Turn verbosity off\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=0)\n",
    "\n",
    "#Test the quality of the fit vs training set size.\n",
    "set_sizes=[50, 200, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "\n",
    "#Record as fcn of set_size\n",
    "clfC = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "best_score = [[0 for i in range(len(set_sizes))] for j in range(n_trainsets)]\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "\n",
    "for ss_idx, ss in enumerate(set_sizes):\n",
    "    for k in range(n_trainsets):\n",
    "        t_corp, y_vals = make_training_set(ss, j_val, result_by_journal)\n",
    "        grid_search.fit(t_corp, y_vals)\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        best_score[k][ss_idx] = grid_search.best_score_\n",
    "        clfC[k][ss_idx] = best_parameters['clf__C']\n",
    "\n",
    "        #print(\"Best score for size %g: %0.3f\" % (ss, grid_search.best_score_)\n",
    "\n",
    "#Plotting\n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, best_score[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Best fit score')\n",
    "title('Nature: Performance of best fit (four iterations)')\n",
    "show()\n",
    "yscale('log')\n",
    "for k in range(n_trainsets):\n",
    "    plot(set_sizes, clfC[k])\n",
    "xlabel('Training set size/3')\n",
    "ylabel('Regularization parameter')\n",
    "title('Nature: Regularization of best fit (four iterations)')\n",
    "show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
