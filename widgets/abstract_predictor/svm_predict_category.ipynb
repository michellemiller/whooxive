{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fancy machine learning to predict whether an article makes it into Nature/Science or PRL. This time we'll only look at articles in the physics.atom-ph section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import combinations\n",
    "import json\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "# from nltk.corpus import stopwords\n",
    "# s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['atom-ph', 'quant-ph', 'cond-mat', 'quant-gas', 'hep-th', 'hep-ex']\n",
    "# categories = ['atom-ph', 'quant-ph']\n",
    "journals_dict = {'PRL': ['Physics Review Letters%',\n",
    "                    'Phys. Rev. Lett.%',\n",
    "                    'Phys.Rev.Lett.%',\n",
    "                    'PRL%'],\n",
    "                 'PR':  ['Physics Review%',\n",
    "                         'Phys. Rev.%',\n",
    "                         'Phys.Rev.%',\n",
    "                         'PR%'],\n",
    "                 'Nature': ['Nature%',\n",
    "                            'Nat.%',\n",
    "                            'Science%'],\n",
    "                 'APL': ['APL%',\n",
    "                         'Appl.Phys.Lett.%',\n",
    "                         'Appl. Phys. Lett.%',\n",
    "                         'Applied Physics Letters%'],\n",
    "                 'AP': ['AP%',\n",
    "                        'Appl.Phys.%',\n",
    "                        'Appl. Phys.%',\n",
    "                        'Applied Physics%'],\n",
    "                 'PL': ['Physics Letters%',\n",
    "                        'Phys. Lett.%',\n",
    "                        'Phys.Lett.%'],\n",
    "                 'All': ['%'],\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_abstracts(category):\n",
    "    query = session.query(Article_Category)\\\n",
    "                    .join(Category)\\\n",
    "                    .join(Article)\\\n",
    "                    .filter(Category.name.like('%' + category + '%'),\n",
    "                            Article.journal_ref.like('Phys.Rev.Lett.%'))\n",
    "                \n",
    "    # Don't need to clean up text: CountVectorizer will do everything\n",
    "    return query\n",
    "#     return [ result.article.abstract for result in query ]\n",
    "\n",
    "\n",
    "# def learn(journal, categories):\n",
    "#     abstracts1 = get_abstracts(journals[0], category)\n",
    "#     abstracts2 = get_abstracts(journals[1], category)\n",
    "    \n",
    "#     half_test_size = int(round(0.2*min(len(abstracts1),len(abstracts2))))\n",
    "# #     print half_test_size\n",
    "\n",
    "#     X1_train, X1_test, y1_train, y1_test = train_test_split(abstracts1, [0]*len(abstracts1), test_size=half_test_size, random_state=42)\n",
    "#     X2_train, X2_test, y2_train, y2_test = train_test_split(abstracts2, [1]*len(abstracts2), test_size=half_test_size, random_state=42)\n",
    "\n",
    "#     X_train = X1_train + X2_train\n",
    "#     X_test = X1_test + X2_test\n",
    "#     y_train = np.array(y1_train + y2_train)\n",
    "#     y_test = np.array(y1_test + y2_test)\n",
    "#     target_names = journals\n",
    "    \n",
    "    \n",
    "#     clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                     ('tfidf', TfidfTransformer()),\n",
    "#                     ('clf', OneVsRestClassifier(\n",
    "#                                 LinearSVC(C=1,penalty='l1',dual=False,fit_intercept=True)))])\n",
    "#     transform = clf.fit_transform(X_train, y_train)\n",
    "#     y_predict_train = clf.predict(X_train)\n",
    "#     y_predict_test = clf.predict(X_test)\n",
    "    \n",
    "#     X_train_tfidf = clf.named_steps['tfidf'].transform(\n",
    "#                         clf.named_steps['vect'].transform(X_train))\n",
    "    \n",
    "#     print (metrics.classification_report(y_test, y_predict_test,\n",
    "#                                     target_names=target_names))\n",
    "# #                                     target_names=test_target_names))\n",
    "#     print metrics.confusion_matrix(y_test, y_predict_test)\n",
    "#     print 'Accuracy: %f' % (metrics.accuracy_score(y_test, y_predict_test))\n",
    "\n",
    "# #     most_important_words = clf.named_steps['clf'].coef_.argsort()[:, ::-1]\n",
    "\n",
    "# #     print np.squeeze(X_train_tfidf[y_train==0, 3].toarray()).shape\n",
    "# #     print np.squeeze(X_train_tfidf[y_train==0, 3].toarray()).shape\n",
    "    \n",
    "    \n",
    "# #     terms =  clf.named_steps['vect'].get_feature_names()\n",
    "# #     result = [{'name': terms[word],\n",
    "# #                'value': clf.named_steps['clf'].coef_[0,word],\n",
    "# #                'vector1': np.squeeze(X_train_tfidf[y_train==0, word].toarray()).tolist(),\n",
    "# #                'vector2': np.squeeze(X_train_tfidf[y_train==1, word].toarray()).tolist(),\n",
    "# #               }\n",
    "# #                 for word in (np.concatenate((most_important_words[0, :15],\n",
    "# #                              most_important_words[0, -15:]))) ]\n",
    "#     return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.29200601578\n"
     ]
    }
   ],
   "source": [
    "# Some abstract have multiple categories. I'll make a dict based on the article\n",
    "# id number to link these labels.\n",
    "start = time.time()\n",
    "\n",
    "query_list = []\n",
    "for category in categories:\n",
    "    query_list.append(get_abstracts(category))\n",
    "    \n",
    "abstract_dict = dict()\n",
    "category_dict = dict()\n",
    "category_to_number = dict(zip(categories, range(0, len(categories))))\n",
    "\n",
    "\n",
    "for q, category in zip(query_list, categories):\n",
    "    for x in q:\n",
    "        abstract_dict[x.article.id] = x.article.abstract\n",
    "        try:\n",
    "            category_dict[x.article.id].append(category)\n",
    "        except KeyError:\n",
    "            category_dict[x.article.id] = [category]\n",
    "keys = abstract_dict.keys()\n",
    "\n",
    "X_train = [abstract_dict[key] for key in keys]\n",
    "Y_train = [ category_dict[key] for key in keys]\n",
    "Y_train = [ [category_to_number[x] for x in category_dict[key]] for key in keys]\n",
    "\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a752c1085c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     ('clf', LinearSVC(C=1,penalty='l1',dual=False,fit_intercept=True))]))\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf_category\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# overall.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'multioutput'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             raise ValueError(\"Multioutput target data is not supported with \"\n",
      "\u001b[0;32m/Users/emarti/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.pyc\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    249\u001b[0m         if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)\n\u001b[1;32m    250\u001b[0m                 and not isinstance(y[0], string_types)):\n\u001b[0;32m--> 251\u001b[0;31m             raise ValueError('You appear to be using a legacy multi-label data'\n\u001b[0m\u001b[1;32m    252\u001b[0m                              \u001b[0;34m' representation. Sequence of sequences are no'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                              \u001b[0;34m' longer supported; use a binary array or sparse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf_category = OneVsRestClassifier(Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(C=1,penalty='l1',dual=False,fit_intercept=True))]))\n",
    "clf_category.fit(X_train, Y_train)\n",
    "\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_category.predict(['We measure the mass, gap, and magnetic moment of a magnon in the ferromagnetic F=1 spinor Bose-Einstein condensate of 87Rb. We find an unusually heavy magnon mass of 1.038(2)stat(8)sys times the atomic mass, as determined by interfering standing and running coherent magnon waves within the dense and trapped condensed gas. This measurement is shifted significantly from theoretical estimates. The magnon energy gap of h×2.5(1)stat(2)sysHz and the effective magnetic moment of −1.04(2)stat(8)μbare times the atomic magnetic moment are consistent with mean-field predictions. The nonzero energy gap arises from magnetic dipole-dipole interactions.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print metrics.classification_report(Y_train, clf.predict(X_train))\n",
    "#                                     target_names=test_target_names))\n",
    "# print metrics.confusion_matrix(y_test, y_predict_test)\n",
    "# print 'Accuracy: %f' % (metrics.accuracy_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "joblib.dump(clf_category, 'svm_category.pkl', compress=1)\n",
    "\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('category_list.json') as f:\n",
    "    category_list = categories\n",
    "    json.dump(category_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
