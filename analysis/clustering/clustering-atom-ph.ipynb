{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I predict the existence of subfields with some cool unsupervised learning algorithm? \n",
    "\n",
    "For starters, let's just use regular n-grams. A more advanced version would be to look for noun phrases or J&K POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "%matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "# from nltk.corpus import stopwords\n",
    "# s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn import metrics\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'acc-phys', u'adap-org', u'alg-geom', u'ao-sci', u'astro-ph', u'astro-ph.CO', u'astro-ph.EP', u'astro-ph.GA', u'astro-ph.HE', u'astro-ph.IM', u'astro-ph.SR', u'atom-ph', u'bayes-an', u'chao-dyn', u'chem-ph', u'comp-gas', u'cond-mat', u'cond-mat.dis-nn', u'cond-mat.mes-hall', u'cond-mat.mtrl-sci', u'cond-mat.other', u'cond-mat.quant-gas', u'cond-mat.soft', u'cond-mat.stat-mech', u'cond-mat.str-el', u'cond-mat.supr-con', u'cs.AI', u'cs.AR', u'cs.CC', u'cs.CE', u'cs.CG', u'cs.CL', u'cs.CR', u'cs.CV', u'cs.CY', u'cs.DB', u'cs.DC', u'cs.DL', u'cs.DM', u'cs.DS', u'cs.ET', u'cs.FL', u'cs.GL', u'cs.GR', u'cs.GT', u'cs.HC', u'cs.IR', u'cs.IT', u'cs.LG', u'cs.LO', u'cs.MA', u'cs.MM', u'cs.MS', u'cs.NA', u'cs.NE', u'cs.NI', u'cs.OH', u'cs.PF', u'cs.PL', u'cs.RO', u'cs.SC', u'cs.SD', u'cs.SE', u'cs.SI', u'cs.SY', u'dg-ga', u'funct-an', u'gr-qc', u'hep-ex', u'hep-lat', u'hep-ph', u'hep-th', u'math-ph', u'math.AC', u'math.AG', u'math.AP', u'math.AT', u'math.CA', u'math.CO', u'math.CT', u'math.CV', u'math.DG', u'math.DS', u'math.FA', u'math.GM', u'math.GN', u'math.GR', u'math.GT', u'math.HO', u'math.IT', u'math.KT', u'math.LO', u'math.MG', u'math.MP', u'math.NA', u'math.NT', u'math.OA', u'math.OC', u'math.PR', u'math.QA', u'math.RA', u'math.RT', u'math.SG', u'math.SP', u'math.ST', u'mtrl-th', u'nlin.AO', u'nlin.CD', u'nlin.CG', u'nlin.PS', u'nlin.SI', u'nucl-ex', u'nucl-th', u'patt-sol', u'physics.acc-ph', u'physics.ao-ph', u'physics.atm-clus', u'physics.atom-ph', u'physics.bio-ph', u'physics.chem-ph', u'physics.class-ph', u'physics.comp-ph', u'physics.data-an', u'physics.ed-ph', u'physics.flu-dyn', u'physics.gen-ph', u'physics.geo-ph', u'physics.hist-ph', u'physics.ins-det', u'physics.med-ph', u'physics.optics', u'physics.plasm-ph', u'physics.pop-ph', u'physics.soc-ph', u'physics.space-ph', u'plasm-ph', u'q-alg', u'q-bio', u'q-bio.BM', u'q-bio.CB', u'q-bio.GN', u'q-bio.MN', u'q-bio.NC', u'q-bio.OT', u'q-bio.PE', u'q-bio.QM', u'q-bio.SC', u'q-bio.TO', u'q-fin.CP', u'q-fin.EC', u'q-fin.GN', u'q-fin.MF', u'q-fin.PM', u'q-fin.PR', u'q-fin.RM', u'q-fin.ST', u'q-fin.TR', u'quant-ph', u'solv-int', u'stat.AP', u'stat.CO', u'stat.ME', u'stat.ML', u'stat.OT', u'stat.TH', u'supr-con']\n"
     ]
    }
   ],
   "source": [
    "# What are the available categories?\n",
    "categories = sorted([x.name for x in session.query(Category)])\n",
    "print categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1983089447\n"
     ]
    }
   ],
   "source": [
    "abstract_all_tmp = {'category': [], 'abstract': []}\n",
    "# category_list = sorted(['atom-ph', 'quant-ph', 'optics', 'nlin', 'str-el', 'stat'])\n",
    "category_list = sorted(['atom-ph'])\n",
    "category_len = len(category_list)\n",
    "\n",
    "start = time.time()\n",
    "for item in category_list:\n",
    "    query = session.query(Article_Category)\\\n",
    "                        .join(Category)\\\n",
    "                        .join(Article)\\\n",
    "                        .filter(Category.name.like('%' + item + '%'))\n",
    "#     query = session.query(Article_Category)\\\n",
    "#                         .join(Category)\\\n",
    "#                         .join(Article)\\\n",
    "#                         .filter(Category.name.like('%' + item + '%'))\n",
    "    result = [' '.join(x.article.abstract.split()) for x in query]\n",
    "    abstract_all_tmp['abstract'].extend(result)\n",
    "    abstract_all_tmp['category'].extend([item]*len(result))\n",
    "print time.time() - start\n",
    "# for item in query:\n",
    "#     abstract_all['category'].append(item.category.name)\n",
    "#     abstract_all['abstract'].append(' '.join(item.article.abstract.split()))\n",
    "# print time.time() - start\n",
    "# abstract_all['atom-ph'] = [x.article.abstract for x in query.all()]\n",
    "# session.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom-ph        9156\n",
      "Total          9156\n"
     ]
    }
   ],
   "source": [
    "# Breakdown of categories?\n",
    "count = Counter(abstract_all_tmp['category'])\n",
    "for key, val in count.iteritems():\n",
    "    print '{:<15}{}'.format(key, val)\n",
    "print '{:<15}{}'.format('Total', len(abstract_all_tmp['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_all = abstract_all_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##Oops! How many overlapping articles do we have? I forgot that arXiv categories aren't unique.\n",
    "# # Let's remove all duplicates.\n",
    "# # This is slow but I am tired.\n",
    "\n",
    "# counter_duplicate = Counter(abstract_all_tmp['abstract'])\n",
    "\n",
    "# abstract_all = {'category': [], 'abstract': []}\n",
    "# for cat, abstract in itertools.izip(abstract_all_tmp['category'], abstract_all_tmp['abstract']):\n",
    "#     if counter_duplicate[abstract] == 1:\n",
    "#         abstract_all['category'].append(cat)\n",
    "#         abstract_all['abstract'].append(abstract)\n",
    "# print len(abstract_all['category'])\n",
    "# print len(abstract_all['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Breakdown of categories? That's a lot of repetition!!!\n",
    "# count = Counter(abstract_all['category'])\n",
    "# for key, val in count.iteritems():\n",
    "#     print '{:<15}{}'.format(key, val)\n",
    "# print '{:<15}{}'.format('Total', len(abstract_all['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train on 80% of the data. Random_state ensures that we always get the same result.\n",
    "x_train, x_test, y_train, y_test = train_test_split(abstract_all['abstract'],\n",
    "                                                    abstract_all['category'],\n",
    "                                                    random_state=42,\n",
    "                                                    train_size=0.8)\n",
    "\n",
    "counter_train = Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, I lied, I'm starting with supervised learning (as a comparison). We're looking at ~60-70% accuracy for these cateogories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #SVC(kernel='linear') is good\n",
    "# clf_supervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                            ('tfidf', TfidfTransformer()),\n",
    "# #                            ('clf', LinearSVC())])\n",
    "#                            ('clf', LinearSVC(C=1,penalty='l1',dual=False,))])\n",
    "# start = time.time()\n",
    "# clf_supervised.fit(x_train, y_train)\n",
    "# print time.time() - start\n",
    "\n",
    "# start = time.time()\n",
    "# predict = clf_supervised.predict(x_test)\n",
    "# print time.time() - start\n",
    "# #print text_abstract_clf.predict(train_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, predict))\n",
    "# print('Accuracy score: %0.2f' % accuracy_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "# most_important_words = clf_supervised.named_steps['clf'].coef_.argsort()[:, ::-1]\n",
    "\n",
    "# terms =  clf_supervised.named_steps['vect'].get_feature_names()\n",
    "# for i in range(len(category_list)):\n",
    "#     print \"Category %s:\" % (category_list[i])\n",
    "#     print ', '.join([terms[x] for x in most_important_words[i, :20]])\n",
    "#     print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try KMeans clustering. \n",
    "See: http://scikit-learn.org/stable/auto_examples/text/document_clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.812951088\n",
      "3.95964097977\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "# Reduce n_init to 10 for testing purposes.\n",
    "clf_unsupervised = Pipeline([('vect', CountVectorizer(ngram_range=(1,3), stop_words='english')),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', KMeans(n_clusters=n_clusters, n_init=10))])\n",
    "start = time.time()\n",
    "clf_unsupervised.fit(x_train)\n",
    "print time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "predict_train = clf_unsupervised.predict(x_train)\n",
    "predict = clf_unsupervised.predict(x_test)\n",
    "print time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Which clusters most closely align with which the original categories?\n",
    "# # Find the strongest correlation, and assign that cluster to the category. Iterate.\n",
    "# # matrix_train = [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))\n",
    "# #                    for x in range(0,n_clusters)] \n",
    "# #                    for cat in category_list]\n",
    "\n",
    "\n",
    "# counter_category = Counter(y_train)\n",
    "# counter_cluster = Counter(predict_train)\n",
    "# accuracy_train_initial = np.array(\n",
    "# #     [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "#     [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_category[cat]\n",
    "#            for x in range(0,n_clusters)] \n",
    "#            for cat in category_list])\n",
    "# clusterToCategory = dict()\n",
    "# # categoryToCluster = dict()\n",
    "\n",
    "# for cluster, item in enumerate(np.argmax(accuracy_train_initial, axis=0).tolist()):\n",
    "#     clusterToCategory[cluster] = category_list[item]\n",
    "\n",
    "\n",
    "# # category_list_remaining = list(category_list)\n",
    "# # cluster_list_remaining = range(0, n_clusters)\n",
    "# # for x in range(0, len(category_list)):\n",
    "# #     accuracy_train = np.array(\n",
    "# #                         [[sum((a==cat and b==x for a,b in zip(y_train, predict_train)))*1./counter_cluster[x]\n",
    "# #                            for x in cluster_list_remaining] \n",
    "# #                            for cat in category_list_remaining])\n",
    "\n",
    "    \n",
    "# #     # Find largest value in the category axis\n",
    "# #     cat_ind, cluster_ind = np.unravel_index(np.argmax(accuracy_train), accuracy_train.shape)\n",
    "# #     clusterToCategory[cluster_list_remaining[cluster_ind]] = category_list_remaining[cat_ind]\n",
    "# #     categoryToCluster[category_list_remaining[cat_ind]] = cluster_list_remaining[cluster_ind]\n",
    "    \n",
    "# #     # Remove those entries from the lists.\n",
    "# #     category_list_remaining.remove(category_list_remaining[cat_ind])\n",
    "# #     cluster_list_remaining.remove(cluster_list_remaining[cluster_ind])\n",
    "# # #     break\n",
    "\n",
    "# # # The remaining clusters predict empty strings\n",
    "# # for item in cluster_list_remaining:\n",
    "# #     clusterToCategory[item] = ''\n",
    "# # clusterToCategory_list = sorted(clusterToCategory.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # The table is normalized by number of elements in each cluster.\n",
    "# print 'Training data'\n",
    "# print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "# print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "# for cat, item in zip(category_list, accuracy_train_initial):\n",
    "#     print ('{:<10}' + '{:<10.2}'*n_clusters).format(cat, *item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Is there overlap between the clusters and existing categories ('ground truth')?\n",
    "# matrix = [[sum((a==cat and b==x for a,b in zip(y_test, predict)))\n",
    "#            for x in range(0,n_clusters)] \n",
    "#            for cat in category_list]\n",
    "\n",
    "# print 'Test data'\n",
    "# print ('{:<10}' + '{:<10}'  *n_clusters).format('', *range(0, n_clusters))\n",
    "# print ('{:<10}' + '{:<10}'  *n_clusters).format('', *[clusterToCategory[x] for x in range(0, n_clusters)])\n",
    "# for cat, item in zip(category_list, matrix):\n",
    "#     print ('{:<10}' + '{:<10}'*n_clusters).format(cat, *item)\n",
    "    \n",
    "    \n",
    "# # # Oops, this is the same as the confusion matrix?\n",
    "# # tmp_reverse_category = dict([(y,x) for x,y in enumerate(category_list)])\n",
    "# # y_test_num = [tmp_reverse_category[x] for x in y_test]\n",
    "# # print ''\n",
    "# # print 'Confusion matrix:'\n",
    "# # print confusion_matrix(y_test_num, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # We can now make a prediction based on these categories.\n",
    "# predict_category = [clusterToCategory[y] for y in predict]\n",
    "\n",
    "# print(classification_report(y_test, predict_category))\n",
    "# print('Accuracy score: %0.2f' % accuracy_score(y_test, predict_category))\n",
    "# print confusion_matrix(y_test, predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "electron, electrons, energy, atom, states, ions, beta, angular, relativistic, decay, atoms, momentum, nuclear, functions, hydrogen, bound, charge, results, photon, ion\n",
      "\n",
      "Cluster 1:\n",
      "quantum, atom, atoms, atomic, light, optical, phase, states, time, wave, cavity, state, systems, single, non, bose, photon, lattice, spin, using\n",
      "\n",
      "Cluster 2:\n",
      "trap, cooling, atoms, ion, optical, laser, trapping, ions, beam, trapped, atom, magnetic, traps, mot, loading, cold, single, magneto, molecules, optical trap\n",
      "\n",
      "Cluster 3:\n",
      "state, ground, ground state, molecules, excited, laser, transition, vibrational, states, hyperfine, excited state, nm, levels, sigma, level, photoassociation, rotational, photon, molecular, using\n",
      "\n",
      "Cluster 4:\n",
      "alpha, nuclear, calculations, relativistic, electron, results, structure, corrections, hyperfine, data, levels, ions, transitions, effects, electric, dipole, order, atomic, fine, values\n",
      "\n",
      "Cluster 5:\n",
      "field, magnetic, magnetic field, spin, fields, electric, atomic, atoms, induced, laser, frequency, eit, probe, optical, magnetic fields, state, resonance, transparency, states, using\n",
      "\n",
      "Cluster 6:\n",
      "cross, energy, scattering, cross sections, sections, body, electron, energies, section, cross section, range, potential, resonances, states, wave, results, method, recombination, calculations, state\n",
      "\n",
      "Cluster 7:\n",
      "ionization, laser, electron, pulse, time, field, harmonic, pulses, strong, attosecond, dependent, time dependent, hhg, strong field, high, generation, intense, laser field, harmonic generation, intensity\n",
      "\n",
      "Cluster 8:\n",
      "frequency, clock, optical, laser, clocks, transition, 10, shift, atomic, lattice, comb, nm, cavity, stability, clock transition, spectroscopy, frequency comb, uncertainty, noise, transitions\n",
      "\n",
      "Cluster 9:\n",
      "rydberg, atoms, states, rydberg atoms, rydberg states, dipole, excitation, interactions, state, interaction, atom, quantum, blockade, dipole dipole, electric, field, rydberg state, excited, dynamics, laser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most important chunks. See http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "order_centroids = clf_unsupervised.named_steps['clf'].cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms =  clf_unsupervised.named_steps['vect'].get_feature_names()\n",
    "for i in range(n_clusters):\n",
    "    print \"Cluster %d:\" % (i)\n",
    "    print ', '.join([terms[x] for x in order_centroids[i, :20]])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is another interesting approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf_pca = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "#                              ('tfidf', TfidfTransformer()),\n",
    "#                              ('clf', KMeans(n_components=6))])\n",
    "# X = clf_pca.fit(x_train)\n",
    "# # predict = clf_pca.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
