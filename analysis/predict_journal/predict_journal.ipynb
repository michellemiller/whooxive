{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fancy machine learning to predict whether an article makes it into Nature/Science or PRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to add parent directoy to sys.path to find 'metadataDB'\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Natural language processing toolkit\n",
    "# To use this, run nltk.download() and download 'stopwords'\n",
    "from nltk.corpus import stopwords\n",
    "s=stopwords.words('english') + ['']\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from metadataDB.declareDatabase import *\n",
    "from sqlalchemy import or_, and_\n",
    "\n",
    "engine = create_engine(\"sqlite:///../arXiv_metadata.db\", echo=False)\n",
    "Base.metadata.bind = engine\n",
    "DBsession = sessionmaker(bind=engine)\n",
    "session = DBsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = session.query(Article_Category)\\\n",
    "                    .join(Category)\\\n",
    "                    .join(Article)\\\n",
    "                    .filter(or_(Article.journal_ref.like('Physics Review Letters%'),\n",
    "                                Article.journal_ref.like('Phys. Rev. Lett.%'),\n",
    "                                Article.journal_ref.like('PRL%')))\n",
    "abstractPRL = [x.article.abstract for x in query.all()]\n",
    "titlePRL = [x.article.title for x in query.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = session.query(Article_Category)\\\n",
    "                    .join(Category)\\\n",
    "                    .join(Article)\\\n",
    "                    .filter(or_(Article.journal_ref.like('Nature%'),\n",
    "                                Article.journal_ref.like('Nat.%'),\n",
    "                                Article.journal_ref.like('Science%')))\n",
    "abstractNatureScience = [x.article.abstract for x in query.all()]\n",
    "titleNatureScience = [x.article.title for x in query.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train with 80% of the data, test with 20%\n",
    "# First start with abstracts.\n",
    "\n",
    "indPRL = len(abstractPRL)*4/5\n",
    "indNatureScience = len(abstractNatureScience)*4/5\n",
    "\n",
    "train_abstract = abstractPRL[:indPRL] + abstractNatureScience[:indNatureScience]\n",
    "train_title = titlePRL[:indPRL] + titleNatureScience[:indNatureScience]\n",
    "train_target = [0]*indPRL + [1]*indNatureScience\n",
    "train_target_names = ['PRL', 'Nature']\n",
    "\n",
    "test_abstract = abstractPRL[indPRL:] + abstractNatureScience[indNatureScience:]\n",
    "test_title = titlePRL[indPRL:] + titleNatureScience[indNatureScience:]\n",
    "test_target = [0]*len(abstractPRL[indPRL:]) + [1]*len(abstractNatureScience[indNatureScience:])\n",
    "test_target_names = ['PRL', 'Nature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "text_abstract_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                              ('tfidf', TfidfTransformer()),\n",
    "                              ('clf', LinearSVC())])\n",
    "text_abstract_clf.fit(train_abstract, train_target)\n",
    "predict_abstract = text_abstract_clf.predict(test_abstract)\n",
    "print text_abstract_clf.predict(train_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "text_title_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                              ('tfidf', TfidfTransformer()),\n",
    "                              ('clf', LinearSVC())])\n",
    "text_title_clf.fit(train_title, train_target)\n",
    "predict_title = text_title_clf.predict(test_title)\n",
    "print text_abstract_clf.predict(train_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 1 1 1]\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print predict_abstract\n",
    "print predict_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        PRL       0.94      0.99      0.96      1604\n",
      "     Nature       0.86      0.59      0.70       237\n",
      "\n",
      "avg / total       0.93      0.93      0.93      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_target, predict_abstract,\n",
    "                                    target_names=test_target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        PRL       0.91      0.97      0.94      1604\n",
      "     Nature       0.66      0.37      0.48       237\n",
      "\n",
      "avg / total       0.88      0.89      0.88      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_target, predict_title,\n",
    "                                    target_names=test_target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2d36794680d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Want to sort dictionary by values. Convert it to a list of tuples so we can use python's sort functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mabstract_wordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtitle_wordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abstract' is not defined"
     ]
    }
   ],
   "source": [
    "# Want to sort dictionary by values. Convert it to a list of tuples so we can use python's sort functions\n",
    "abstract_wordlist = sorted([(key, val) for key, val in abstract.iteritems()], key=lambda x: x[1])\n",
    "title_wordlist = sorted([(key, val) for key, val in title.iteritems()], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstract_best_and_worst = abstract_wordlist[0:10] + abstract_wordlist[-11:-1]\n",
    "print abstract_best_and_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_best_and_worst = title_wordlist[0:10] + title_wordlist[-11:-1]\n",
    "print title_best_and_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [x for (x,y) in abstract_best_and_worst]\n",
    "vals = np.array([y for (x,y) in abstract_best_and_worst])\n",
    "\n",
    "\n",
    "ind = np.arange(0, len(words))\n",
    "c = ['b' if x > 0 else 'r' for x in vals]\n",
    "\n",
    "plt.figure(frameon=False, figsize=(6,8))\n",
    "plt.barh(ind - 0.5,\n",
    "         vals,\n",
    "         color=c)\n",
    "plt.yticks(ind, words)\n",
    "plt.ylim(ind[0]-1, ind[-1]+1)\n",
    "plt.xlim(-1.1*np.max(np.abs(vals)), 1.1*np.max(np.abs(vals)))\n",
    "plt.title('Abstracts')\n",
    "plt.xlabel('PRL$\\qquad\\qquad\\qquad\\qquad$Nature/Science')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [x for (x,y) in title_best_and_worst]\n",
    "vals = np.array([y for (x,y) in title_best_and_worst])\n",
    "\n",
    "\n",
    "ind = np.arange(0, len(words))\n",
    "c = ['b' if x > 0 else 'r' for x in vals]\n",
    "\n",
    "plt.figure(frameon=False, figsize=(6,8))\n",
    "plt.barh(ind - 0.5,\n",
    "         vals,\n",
    "         color=c)\n",
    "plt.yticks(ind, words)\n",
    "plt.ylim(ind[0]-1, ind[-1]+1)\n",
    "plt.xlim(-1.1*np.max(np.abs(vals)), 1.1*np.max(np.abs(vals)))\n",
    "plt.title('Titles')\n",
    "plt.xlabel('PRL$\\qquad\\qquad\\qquad\\qquad$Nature/Science')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above plots are fun but they are descriptive, not predictive. Given words in the abstract or title, can I predict where an article will end up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in resultNatureScience:\n",
    "    print \"%s - %s\" % (item.article.journal_ref, item.category.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
